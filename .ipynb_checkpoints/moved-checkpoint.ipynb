{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_in_k_encoding(vec, k):\n",
    "    \"\"\" One-in-k encoding of vector to k classes \n",
    "    \n",
    "    Args:\n",
    "       vec: numpy array - data to encode\n",
    "       k: int - number of classes to encode to (0,...,k-1)\n",
    "    \"\"\"\n",
    "    n = vec.shape[0]\n",
    "    enc = np.zeros((n, k))\n",
    "    enc[np.arange(n), vec] = 1\n",
    "    return enc\n",
    "\n",
    "def softmax(X):\n",
    "    \"\"\" \n",
    "    Compute the softmax of each row of an input matrix (2D numpy array). You can take this from handin I\n",
    "    \n",
    "    the numpy functions amax, log, exp, sum may come in handy as well as the keepdims=True option and the axis option.\n",
    "    Remember to handle the numerical problems as discussed in the description.\n",
    "    You should compute lg softmax first and then exponentiate \n",
    "    \n",
    "    More precisely this is what you must do.\n",
    "    \n",
    "    For each row x do:\n",
    "    compute max of x\n",
    "    compute the log of the denominator sum for softmax but subtracting out the max i.e (log sum exp x-max) + max\n",
    "    compute log of the softmax: x - logsum\n",
    "    exponentiate that\n",
    "    \n",
    "    You can do all of it without for loops using numpys vectorized operations.\n",
    "\n",
    "    Args:\n",
    "        X: numpy array shape (n, d) each row is a data point\n",
    "    Returns:\n",
    "        res: numpy array shape (n, d)  where each row is the softmax transformation of the corresponding row in X i.e res[i, :] = softmax(X[i, :])\n",
    "    \"\"\"\n",
    "    res = np.zeros(X.shape)\n",
    "    ### YOUR CODE HERE no for loops please\n",
    "    mx = np.amax(X, axis=1, keepdims=True)\n",
    "    tmp = np.log(np.sum(np.exp(X - mx), axis=1, keepdims=True)) + mx\n",
    "    res =  np.exp(X - tmp)\n",
    "    ### END CODE\n",
    "    return res\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\" Compute the relu activation function on every element of the input\n",
    "    \n",
    "        Args:\n",
    "            x: np.array\n",
    "        Returns:\n",
    "            res: np.array same shape as x\n",
    "        Beware of np.max and look at np.maximum\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    res =  np.maximum(0, x)\n",
    "    ### END CODE\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_init_params(input_dim, hidden_size, output_size):\n",
    "    \"\"\" Simple initializer function\"\"\"\n",
    "    W1 = np.random.randn(input_dim, hidden_size)\n",
    "    b1 = np.random.randn(1, hidden_size) \n",
    "    W2 = np.random.randn(hidden_size, output_size)\n",
    "    b2 = np.random.randn(1, output_size)\n",
    "    return {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "    \n",
    "class NetClassifier():\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_size, output_size):\n",
    "        \"\"\" Init self.W1, self.b1, self.W2, self. b2 using np.random.randn\"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        ### YOUR CODE HERE\n",
    "        self.W1 = np.random.rand(input_dim, hidden_size)\n",
    "        self.b1 = np.random.rand(1, hidden_size) \n",
    "        self.W2 = np.random.rand(hidden_size, output_size)\n",
    "        self.b2 = np.random.rand(1, output_size)\n",
    "        ### END CODE    \n",
    "        print('Neural Net Shapes', self.W1.shape, self.b1.shape, self.W2.shape, self.b2.shape)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = None\n",
    "        ### YOUR CODE HERE\n",
    "        hin = X @ self.W1 + self.b1\n",
    "        hout = relu(hin)\n",
    "        sin = hout @ self.W2 + b2\n",
    "        pred = np.argmax(X, axis=1)\n",
    "        ### END CODE\n",
    "        return pred\n",
    "     \n",
    "    def score(self, X, y):\n",
    "        acc = None\n",
    "        ### YOUR CODE HERE\n",
    "        hin = X @ self.W1 + self.b1\n",
    "        hout = relu(hin)\n",
    "        sin = hout @ self.W2 + b2\n",
    "        pred = self.predict(X)\n",
    "        acc = (pred == y).mean()\n",
    "        ### END CODE\n",
    "        return acc\n",
    "    \n",
    "    @staticmethod\n",
    "    def cost_grad(X, y, params, c=0.0):\n",
    "        \"\"\" Compute cost and gradient of neural net on data X with labels y using weight decay parameter c\n",
    "        You should implement a forward pass and store the intermediate results \n",
    "        and the implement the backwards pass using the intermediate stored results\n",
    "        \n",
    "        Use the derivative for cost as a function for input to softmax as derived above\n",
    "        \n",
    "        Args:\n",
    "            X: np.array shape n, self.input_size\n",
    "            y: np.array shape n, \n",
    "            params: dict with keys (W1, W2, b1, b2)\n",
    "            c: float - weight decay regularization weight\n",
    "        \n",
    "        Returns \n",
    "            cost: scalar - average cross entropy cost\n",
    "            dict with keys\n",
    "            d_w1: np.array shape w1.shape, entry d_w1[i, j] = \\partial cost/ \\partial w1[i, j]\n",
    "            d_w2: np.array shape w2.shape, entry d_w2[i, j] = \\partial cost/ \\partial w2[i, j]\n",
    "            d_b1: np.array shape b1.shape, entry d_b1[1, j] = \\partial cost/ \\partial b1[1, j]\n",
    "            d_b2: np.array shape b2.shape, entry d_b2[1, j] = \\partial cost/ \\partial b2[1, j]\n",
    "            \n",
    "        \"\"\"\n",
    "        n = X.shape[0] \n",
    "        W1 = params['W1']\n",
    "        b1 = params['b1']\n",
    "        W2 = params['W2']\n",
    "        b2 = params['b2']\n",
    "        labels = one_in_k_encoding(Y, W2.shape[1]) # shape n x k\n",
    "\n",
    "        \n",
    "        ### YOUR CODE HERE - FORWARD PASS - compute unnormalized cost and store relevant values for backprop\n",
    "        hin = X @ W1 + b1           # shape n x hidden_size\n",
    "        hout = relu(hin)                      # shape n x hidden_size\n",
    "        soft_in = hout @ W2 + b2    # shape n x output + size\n",
    "        sm = softmax(soft_in)                 # shape m x output_size\n",
    "        # cost = - np.sum(np.log (np.choose(sm, Y))) # scalar\n",
    "        cost = -np.sum(np.log(sm[np.arange(y.shape[0]), y]))\n",
    "    \n",
    "        log_sum = (1/(np.sum(np.exp(soft_in), axis=1, keepdims=True)))*np.exp(soft_in)        \n",
    "        assert np.allclose(log_sum, sm), ('i was wrong...', log_sum.shape, soft_in.shape, log_sum - soft_in)\n",
    "        ### END CODE\n",
    "        \n",
    "        ### YOUR CODE HERE - BACKWARDS PASS - compute unnormalized derivatives of all weights and bias, store them in d_w1, d_w2' d_w2, d_b1, d_b2\n",
    "        J_cost_softin_all = - labels + sm # Jacobian of cost after softin for every input point (each row has one), shape n x output_size \n",
    "        #d_softin_rows = - labels + (1/(np.sum(np.exp(softin), axis=1, keepdims=True)))*np.exp(softin) # M x Dy\n",
    "        #d_softin_rows = - labels + softin\n",
    "        J_cost_b2 = np.sum(J_cost_softin_all, axis=0, keepdims=True) # shape 1 x self.hidden we sum over all data points (rows) to get sum of derivatives of b\n",
    "        # shape h x output_size as required, a clever way of using mat mult to compute the sum of all derivatives \n",
    "        J_cost_w2 = hout.T @ J_cost_softin_all \n",
    "        J_cost_hout_all = J_cost_softin_all @ W2.T # shape n x hidden_size - makes sense\n",
    "        J_cost_hin_all = J_cost_hout_all.copy()\n",
    "        J_cost_hin_all[hin < 0 ] = 0\n",
    "        J_cost_b1 = np.sum(J_cost_hin_all, axis=0, keepdims=True)\n",
    "        J_cost_w1 = X.T @ J_cost_hin_all\n",
    "        ## Compute weight decay \n",
    "        reg_cost = c * (np.sum(W1*W1) + np.sum(W2*W2))\n",
    "        print('reg cost', c, reg_cost)\n",
    "        d_reg_w1 = 2.0 * c * W1 \n",
    "        d_reg_w2 = 2.0 * c * W2\n",
    "        ## Store the vectors\n",
    "        d_w1 = J_cost_w1\n",
    "        d_w2 = J_cost_w2\n",
    "        d_b1 = J_cost_b1\n",
    "        d_b2 = J_cost_b2\n",
    "        print('derivative shapes', d_w1.shape, d_w2.shape, d_b1.shape, d_b2.shape)\n",
    "        assert d_w1.shape == W1.shape\n",
    "        assert d_w2.shape == W2.shape\n",
    "        assert d_b1.shape == b1.shape\n",
    "        assert d_b2.shape == b2.shape\n",
    "        \n",
    "        return cost/n + reg_cost, {'d_w1': d_w1/n + d_reg_w1, 'd_w2': d_w2/n + d_reg_w2, 'd_b1': d_b1/n, 'd_b2': d_b2/n}\n",
    "        ### END CODE\n",
    "        \n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val, y_val, init_params, batch_size=32, lr=0.1, C=1e-4, epochs=30):\n",
    "        \"\"\" Run Mini-Batch Gradient Descent on data X,Y to minimize the in sample error (1/n)Cross Entropy for Neural Net classification\n",
    "        Printing the performance every epoch is a good idea to see if the algorithm is working\n",
    "    \n",
    "        Args:\n",
    "           X_train: numpy array shape (n, d) - the training data each row is a data point\n",
    "           y_train: numpy array shape (n,) int - training target labels numbers in {0, 1,..., k-1}\n",
    "           X_val: numpy array shape (n, d) - the validation data each row is a data point\n",
    "           y_val: numpy array shape (n,) int - validation target labels numbers in {0, 1,..., k-1}\n",
    "           params: dict - has initial setting of parameters - you are allowed to owerwrite these\n",
    "           lr: scalar - initial learning rate\n",
    "           batchsize: scalar - size of mini-batch\n",
    "           epochs: scalar - number of iterations through the data to use\n",
    "\n",
    "        Sets: \n",
    "           W1, W2, b1, b2: parameters for neural net\n",
    "           history: dict:{keys: train_loss, train_acc, val_loss, val_acc} each an np.array of size epochs of the the given cost after every epoch\n",
    "        \"\"\" \n",
    "        history = []\n",
    "        W1 = params['W1']\n",
    "        b1 = params['b1']\n",
    "        W2 = params['W2']\n",
    "        b2 = params['b2']\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "        n = X.shape[0]\n",
    "        best_val_acc_so_far = params\n",
    "        for e in range(epochs):\n",
    "            rp = np.random.permutation(X.shape[0])\n",
    "            rpx = X[rp]\n",
    "            rpy = Y[rp]\n",
    "            for j in range(0, n, batch_size):\n",
    "                l = min(j + batch_size, n)\n",
    "                xchunk = rpx[j:l, :]\n",
    "                ychunk = rpy[j:l]\n",
    "                cost, grad = self.cost_grad(xchunk, ychunk, 0)\n",
    "                W1 = W1 - lr * grad['d_w1']\n",
    "                W2 = W2 - lr * grad['d_w2']\n",
    "                b1 = b1 - lr * grad['d_b1']\n",
    "                b2 = b2 - lr * grad['d_b2']\n",
    "\n",
    "            train_cost, train_grad = self.cost_grad(X_train, y_train, params, C=0)\n",
    "            train_cur_acc = self.score(X_train, y_train)\n",
    "            val_cost, val_grad = self.cost_grad(X_val, y_val, params, C=0)\n",
    "            val_cur_acc = self.score(X_val, y_val)\n",
    "            if val_cur_acc > best_val_acc_so_far:\n",
    "                best_weights = {key: value.copy() for key, value in params.items()}\n",
    "                best_val_acc_so_far = cur_acc\n",
    "            history.append((train_cost, train_acc, val_cost, val_acc))\n",
    "            \n",
    "            lr = lr * 0.99\n",
    "            print('Epoch: {0}, Cost: {1} - new lr: {2}'.format(e, cost, lr))        \n",
    "        hist = {\n",
    "            'train_loss': np.array([x[0] for x in history]),\n",
    "            'train_acc': np.array([x[1] for x in history]),\n",
    "            'val_loss': np.array([x[2] for x in history]),\n",
    "            'val_acc': np.array([x[3] for x in history])            \n",
    "        }\n",
    "        ### END CODE\n",
    "        \n",
    "        self.history = history\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "input_dim = 3\n",
    "hidden_size = 5\n",
    "output_size = 4\n",
    "nc = NetClassifier(input_dim, hidden_size, output_size)\n",
    "params = get_init_params(input_size, hidden_size, output_size)\n",
    "\n",
    "X = np.random.randn(7, input_dim)\n",
    "Y = np.array([0, 1, 2, 0, 1, 2, 0])\n",
    "nc.cost_grad(X, Y, params, c=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_grad_check(f, x, key):\n",
    "    \"\"\" Numerical Gradient Checker \"\"\"\n",
    "    eps = 1e-6\n",
    "    h = 1e-5\n",
    "    # d = x.shape[0]\n",
    "    cost, grad = f(x)\n",
    "    grad = grad[key]\n",
    "    it = np.nditer(x, flags=['multi_index'])\n",
    "    while not it.finished:    \n",
    "        dim = it.multi_index    \n",
    "        print(dim)\n",
    "        tmp = x[dim]\n",
    "        x[dim] = tmp + h\n",
    "        cplus, _ = f(x)\n",
    "        x[dim] = tmp - h \n",
    "        cminus, _ = f(x)\n",
    "        x[dim] = tmp\n",
    "        num_grad = (cplus-cminus)/(2*h)\n",
    "        print('cplus cminus', cplus, cminus, cplus-cminus)\n",
    "        print('dim, grad, num_grad, grad-num_grad', dim, grad[dim], num_grad, grad[dim]-num_grad)\n",
    "        assert np.abs(num_grad - grad[dim]) < eps, 'numerical gradient error index {0}, numerical gradient {1}, computed gradient {2}'.format(dim, num_grad, grad[dim])\n",
    "        it.iternext()\n",
    "\n",
    "def test_grad():\n",
    "    stars = '*'*5\n",
    "    print(stars, 'Testing  Cost and Gradient Together')\n",
    "    input_dim = 7\n",
    "    hidden_size = 1\n",
    "    output_size = 3\n",
    "    nc = NetClassifier(input_dim, hidden_size, output_size)\n",
    "    params = get_init_params(input_dim, hidden_size, output_size)\n",
    "\n",
    "    nc = NetClassifier(input_dim, hidden_size, output_size)\n",
    "    X = np.random.randn(7, input_dim)\n",
    "    y = np.array([0, 1, 2, 0, 1, 2, 0])\n",
    "\n",
    "    f = lambda z: nc.cost_grad(X, y, params, c=1.0)\n",
    "    print('\\n', stars, 'Test Cost and Gradient of b2', stars)\n",
    "    numerical_grad_check(f, params['b2'], 'd_b2')\n",
    "    print(stars, 'Test Success', stars)\n",
    "    \n",
    "    print('\\n', stars, 'Test Cost and Gradient of w2', stars)\n",
    "    numerical_grad_check(f, params['W2'], 'd_w2')\n",
    "    print('Test Success')\n",
    "    \n",
    "    print('\\n', stars, 'Test Cost and Gradient of b1', stars)\n",
    "    numerical_grad_check(f, params['b1'], 'd_b1')\n",
    "    print('Test Success')\n",
    "    \n",
    "    print('\\n', stars, 'Test Cost and Gradient of w1', stars)\n",
    "    numerical_grad_check(f, params['W1'], 'd_w1')\n",
    "    print('Test Success')\n",
    "\n",
    "test_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
